{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FALKOR | Automated Trading Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PyTorch models for stock price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Link: https://github.com/vdyagilev/FALKOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_wrappers.BinanceWrapper import BinanceWrapper\n",
    "from helpers import data_processing\n",
    "from helpers.charting_tools import Charting\n",
    "from helpers.datasets import ChartImageDataset, ArrayTimeSeriesDataset\n",
    "from helpers.saving_models import load_model, save_model\n",
    "\n",
    "from models.CNN.CNN import CNN\n",
    "from models.GRU.GRU import GRUnet\n",
    "from models.GRU_CNN.GRU_CNN import GRU_CNN\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import *\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 'nBjgb83VMNvqq45b3JdWUIsJDalWlXxHI2bvDz9oLdW7KgOLPvJCp30CHnthjfNJ'\n",
    "sec = '5bBN7s7h37kUvmGIpF9FTAtspBY93WirwhTh39PV7AlKSlUE2S4EEe9b3OZVYIqd'\n",
    "\n",
    "binance = BinanceWrapper(id_, sec)\n",
    "\n",
    "symbol = 'ETHBTC'\n",
    "interval = '1m'\n",
    "\n",
    "def p(d1, d2):\n",
    "    return binance.historical_candles(symbol, interval, d1, d2)\n",
    "ochls = [p('April 1 2018', 'May 15 2018'), p('May 15 2018', 'July 1 2018'), p('July 1 2018', 'August 15 2018'), p('August 15 2018', 'October 1 2018'),\n",
    "        p('October 1 2018', 'November 15 2018'), p('November 15 2018', 'January 1 2019'), p('January 1 2019', 'February 15 2019'), p('February 15 2019', 'April 1 2019'),\n",
    "        p('April 1 2019', 'May 15 2019'), p('May 15 2019', 'July 1 2019'), p('July 1 2019', 'August 15 2019')]\n",
    "\n",
    "#ochlv_df = binance.historical_candles(symbol, interval, 'April 1 2018', 'May 15 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(ochlv_df):\n",
    "    ti_df = data_processing.add_ti(ochlv_df)\n",
    "\n",
    "    # return = (future - curr) / curr\n",
    "\n",
    "    price_return = ((ti_df.close)-(ti_df.close.shift(1))) / ti_df.close\n",
    "    volume_return = ((ti_df.volume)-(ti_df.volume.shift(1))) / ti_df.volume\n",
    "\n",
    "    ti_df['price_return'] = price_return\n",
    "    ti_df['volume_return'] = volume_return\n",
    "\n",
    "    dataset_windows = data_processing.split_dataset(ti_df, a=0, b=30, step_size=5)\n",
    "    for df in dataset_windows: df.reset_index(drop=True, inplace=True) # reindex 0-29\n",
    "\n",
    "    # Paths to training images and testing images\n",
    "    train_img_path = Path('models/CNN/training-images/')\n",
    "    image_path_list = [train_img_path / 'image-{}.png'.format(i) for i in range(len(dataset_windows))]\n",
    "\n",
    "    p = multiprocessing.Pool(processes = 6)\n",
    "    p.map_async(generate_chart_image, [i for i in range(len(dataset_windows))])\n",
    "\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    price_labels_dct = data_processing.price_labels(dataset_windows, period_size=30)\n",
    "    price_returns = price_labels_dct['return']\n",
    "\n",
    "    # ensure len(dataset_windows) == len(price_returns)\n",
    "    dataset_windows = dataset_windows[:len(price_returns)]\n",
    "\n",
    "    for i in range(len(dataset_windows)): normalize_data(i, dataset_windows)\n",
    "\n",
    "    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Parameters\n",
    "    params = {'batch_size': 64,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 5}\n",
    "\n",
    "    num_epochs = 5\n",
    "\n",
    "    # Remove all NaN values - in our case only the first window has a NaN so we remove first window and label\n",
    "    dataset_windows = dataset_windows[1:]\n",
    "    price_returns = price_returns[1:] \n",
    "    image_path_list = image_path_list[1:]\n",
    "    curr_prices = price_labels_dct['curr_price'][1:]\n",
    "    future_prices = price_labels_dct['future_price'][1:]\n",
    "\n",
    "    while len(dataset_windows) % params['batch_size'] != 0:\n",
    "        dataset_windows = dataset_windows[:-1]\n",
    "\n",
    "    while len(curr_prices) % params['batch_size'] != 0:\n",
    "        curr_prices = curr_prices[:-1]\n",
    "\n",
    "    while len(future_prices) % params['batch_size'] != 0:\n",
    "        future_prices = future_prices[:-1]\n",
    "\n",
    "    while len(price_returns) % params['batch_size'] != 0:\n",
    "        price_returns = price_returns[:-1]\n",
    "\n",
    "    image_path_list = image_path_list[:len(price_returns)]\n",
    "\n",
    "    assert len(dataset_windows) == len(price_returns) == len(curr_prices) == len(future_prices) == len(image_path_list)\n",
    "\n",
    "    # specify the split between train_df and valid_df from the process of splitting dataset_windows \n",
    "    split = 0.7\n",
    "\n",
    "    s = int(len(dataset_windows) * 0.7)\n",
    "    while s % params['batch_size'] != 0:\n",
    "        s += 1\n",
    "\n",
    "    # create two ChartImageDatasets, split by split, for the purpose of creating a DataLoader for the specific model\n",
    "\n",
    "    train_ds_cnn = ChartImageDataset(image_path_list[:s], price_returns[:s])\n",
    "    valid_ds_cnn = ChartImageDataset(image_path_list[s:], price_returns[s:])\n",
    "\n",
    "    # add potential profit as label\n",
    "    test_ds_cnn = ChartImageDataset(image_path_list[s:], [future_prices[i] - curr_prices[i] for i in range(s, len(future_prices))])\n",
    "\n",
    "    train_gen_cnn = DataLoader(train_ds_cnn, **params)\n",
    "    valid_gen_cnn = DataLoader(valid_ds_cnn, **params)\n",
    "    train_gen_cnn = DataLoader(valid_ds_cnn, **params)\n",
    "\n",
    "    cnn = CNN().cuda()\n",
    "\n",
    "    cnn_path = Path('models/CNN/cnn_weights')\n",
    "    load_model(cnn, cnn_path)\n",
    "\n",
    "    train(cnn, num_epochs, batch_size=params['batch_size'], train_gen=train_gen_cnn, valid_gen=valid_gen_cnn, test_gen=train_gen_cnn)\n",
    "\n",
    "    save_model(cnn, cnn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Training Loss: 0.0034 Validation Loss: 0.0021 Profitability: 4.007\n",
      "Epoch: 2/5... Training Loss: 0.0016 Validation Loss: 0.0011 Profitability: 4.685\n",
      "Epoch: 3/5... Training Loss: 0.0014 Validation Loss: 0.0011 Profitability: 4.68\n",
      "Epoch: 4/5... Training Loss: 0.0012 Validation Loss: 0.0014 Profitability: 4.507\n",
      "Epoch: 5/5... Training Loss: 0.001 Validation Loss: 0.0013 Profitability: 4.567\n",
      "Epoch: 1/5... Training Loss: 0.0023 Validation Loss: 0.0024 Profitability: 2.11\n",
      "Epoch: 2/5... Training Loss: 0.0016 Validation Loss: 0.0023 Profitability: 2.09\n",
      "Epoch: 3/5... Training Loss: 0.0015 Validation Loss: 0.0016 Profitability: 2.775\n",
      "Epoch: 4/5... Training Loss: 0.0014 Validation Loss: 0.0015 Profitability: 3.04\n",
      "Epoch: 5/5... Training Loss: 0.0015 Validation Loss: 0.0014 Profitability: 3.031\n",
      "Epoch: 1/5... Training Loss: 0.0027 Validation Loss: 0.0022 Profitability: 3.58\n",
      "Epoch: 2/5... Training Loss: 0.0015 Validation Loss: 0.0013 Profitability: 4.186\n",
      "Epoch: 3/5... Training Loss: 0.0014 Validation Loss: 0.0014 Profitability: 4.242\n",
      "Epoch: 4/5... Training Loss: 0.0013 Validation Loss: 0.0012 Profitability: 4.398\n",
      "Epoch: 5/5... Training Loss: 0.0013 Validation Loss: 0.0016 Profitability: 4.083\n",
      "Epoch: 1/5... Training Loss: 0.0036 Validation Loss: 0.0028 Profitability: 4.278\n",
      "Epoch: 2/5... Training Loss: 0.0024 Validation Loss: 0.0019 Profitability: 4.967\n",
      "Epoch: 3/5... Training Loss: 0.0021 Validation Loss: 0.0029 Profitability: 4.268\n",
      "Epoch: 4/5... Training Loss: 0.0021 Validation Loss: 0.0029 Profitability: 4.138\n",
      "Epoch: 5/5... Training Loss: 0.0023 Validation Loss: 0.0019 Profitability: 4.927\n",
      "Epoch: 1/5... Training Loss: 0.0024 Validation Loss: 0.0017 Profitability: 1.571\n",
      "Epoch: 2/5... Training Loss: 0.0015 Validation Loss: 0.0013 Profitability: 2.006\n",
      "Epoch: 3/5... Training Loss: 0.0014 Validation Loss: 0.0015 Profitability: 1.677\n",
      "Epoch: 4/5... Training Loss: 0.0015 Validation Loss: 0.0012 Profitability: 2.057\n",
      "Epoch: 5/5... Training Loss: 0.0016 Validation Loss: 0.002 Profitability: 1.246\n",
      "Epoch: 1/5... Training Loss: 0.0048 Validation Loss: 0.0054 Profitability: 3.208\n",
      "Epoch: 2/5... Training Loss: 0.0028 Validation Loss: 0.0025 Profitability: 5.462\n",
      "Epoch: 3/5... Training Loss: 0.0025 Validation Loss: 0.0029 Profitability: 5.092\n",
      "Epoch: 4/5... Training Loss: 0.0026 Validation Loss: 0.0025 Profitability: 5.43\n",
      "Epoch: 5/5... Training Loss: 0.0024 Validation Loss: 0.0024 Profitability: 5.475\n",
      "Epoch: 1/5... Training Loss: 0.0025 Validation Loss: 0.0019 Profitability: 1.256\n",
      "Epoch: 2/5... Training Loss: 0.0013 Validation Loss: 0.0007 Profitability: 2.263\n",
      "Epoch: 3/5... Training Loss: 0.0009 Validation Loss: 0.0012 Profitability: 1.841\n",
      "Epoch: 4/5... Training Loss: 0.0009 Validation Loss: 0.0009 Profitability: 2.171\n",
      "Epoch: 5/5... Training Loss: 0.0009 Validation Loss: 0.0009 Profitability: 2.122\n",
      "Epoch: 1/5... Training Loss: 0.0015 Validation Loss: 0.0011 Profitability: 0.743\n",
      "Epoch: 2/5... Training Loss: 0.0006 Validation Loss: 0.0015 Profitability: 0.66\n",
      "Epoch: 3/5... Training Loss: 0.0006 Validation Loss: 0.0004 Profitability: 1.416\n",
      "Epoch: 4/5... Training Loss: 0.0004 Validation Loss: 0.0013 Profitability: 0.69\n",
      "Epoch: 5/5... Training Loss: 0.0005 Validation Loss: 0.0003 Profitability: 1.433\n",
      "Epoch: 1/5... Training Loss: 0.0034 Validation Loss: 0.0052 Profitability: 3.198\n",
      "Epoch: 2/5... Training Loss: 0.0017 Validation Loss: 0.0009 Profitability: 5.486\n",
      "Epoch: 3/5... Training Loss: 0.0012 Validation Loss: 0.0007 Profitability: 5.533\n",
      "Epoch: 4/5... Training Loss: 0.0011 Validation Loss: 0.002 Profitability: 4.888\n",
      "Epoch: 5/5... Training Loss: 0.0009 Validation Loss: 0.0007 Profitability: 5.521\n",
      "Epoch: 1/5... Training Loss: 0.004 Validation Loss: 0.0036 Profitability: 3.605\n",
      "Epoch: 2/5... Training Loss: 0.0023 Validation Loss: 0.0021 Profitability: 4.688\n",
      "Epoch: 3/5... Training Loss: 0.0023 Validation Loss: 0.0023 Profitability: 4.504\n",
      "Epoch: 4/5... Training Loss: 0.0022 Validation Loss: 0.0019 Profitability: 4.831\n",
      "Epoch: 5/5... Training Loss: 0.0021 Validation Loss: 0.0019 Profitability: 4.826\n",
      "Epoch: 1/5... Training Loss: 0.0026 Validation Loss: 0.0018 Profitability: 2.957\n",
      "Epoch: 2/5... Training Loss: 0.0016 Validation Loss: 0.0015 Profitability: 3.086\n",
      "Epoch: 3/5... Training Loss: 0.0015 Validation Loss: 0.0018 Profitability: 2.93\n",
      "Epoch: 4/5... Training Loss: 0.0015 Validation Loss: 0.0015 Profitability: 3.077\n",
      "Epoch: 5/5... Training Loss: 0.0013 Validation Loss: 0.0012 Profitability: 3.464\n"
     ]
    }
   ],
   "source": [
    "for o in ochls:\n",
    "    train_cnn(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_df = data_processing.add_ti(ochlv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add price returns and volume returns as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return = (future - curr) / curr\n",
    "\n",
    "price_return = ((ti_df.close)-(ti_df.close.shift(1))) / ti_df.close\n",
    "volume_return = ((ti_df.volume)-(ti_df.volume.shift(1))) / ti_df.volume\n",
    "\n",
    "ti_df['price_return'] = price_return\n",
    "ti_df['volume_return'] = volume_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_windows = data_processing.split_dataset(ti_df, a=0, b=30, step_size=5)\n",
    "for df in dataset_windows: df.reset_index(drop=True, inplace=True) # reindex 0-29\n",
    "print(len(dataset_windows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Chart Images for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to training images and testing images\n",
    "train_img_path = Path('models/CNN/training-images/')\n",
    "image_path_list = [train_img_path / 'image-{}.png'.format(i) for i in range(len(dataset_windows))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chart_image(i):\n",
    "    df = dataset_windows[i]\n",
    "    chart = Charting(df=df, col_label='time', row_label='close', tech_inds=['sma20', 'bb20_low', 'bb20_mid', 'bb20_up'])\n",
    "    chart.chart_to_image(train_img_path / 'image-{}.png'.format(i)) # the / is a Path join method \n",
    "    \n",
    "    del chart\n",
    "\n",
    "# p = multiprocessing.Pool(processes = 6)\n",
    "# p.map_async(generate_chart_image, [i for i in range(len(dataset_windows))])\n",
    "\n",
    "# p.close()\n",
    "# p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing input DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_labels_dct = data_processing.price_labels(dataset_windows, period_size=30)\n",
    "price_returns = price_labels_dct['return']\n",
    "\n",
    "# ensure len(dataset_windows) == len(price_returns)\n",
    "dataset_windows = dataset_windows[:len(price_returns)]\n",
    "len(dataset_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize_data(i, dataset_windows):\n",
    "    df = dataset_windows[i]\n",
    "    \n",
    "    # remove all hardcoded features like time open high low close, which have 0 predictive ability\n",
    "    if 'time' in df: df = df.drop('time', axis=1)\n",
    "#     if 'open' in df: df = df.drop('open', axis=1)\n",
    "#     if 'high' in df: df = df.drop('high', axis=1)\n",
    "#     if 'low' in df: df = df.drop('low', axis=1)\n",
    "#     if 'close' in df: df = df.drop('close', axis=1)\n",
    "#     if 'volume' in df: df = df.drop('volume', axis=1)\n",
    "        \n",
    "    df = df.fillna(0) # replace all NaN with 0. \n",
    "   \n",
    "    dataset_windows[i] = df\n",
    "    \n",
    "# for i in range(len(dataset_windows)): normalize_data(i, dataset_windows)\n",
    "# dataset_windows[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 5}\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all NaN values - in our case only the first window has a NaN so we remove first window and label\n",
    "dataset_windows = dataset_windows[1:]\n",
    "price_returns = price_returns[1:] \n",
    "image_path_list = image_path_list[1:]\n",
    "curr_prices = price_labels_dct['curr_price'][1:]\n",
    "future_prices = price_labels_dct['future_price'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(dataset_windows) % params['batch_size'] != 0:\n",
    "    dataset_windows = dataset_windows[:-1]\n",
    "    \n",
    "while len(curr_prices) % params['batch_size'] != 0:\n",
    "    curr_prices = curr_prices[:-1]\n",
    "\n",
    "while len(future_prices) % params['batch_size'] != 0:\n",
    "    future_prices = future_prices[:-1]\n",
    "\n",
    "while len(price_returns) % params['batch_size'] != 0:\n",
    "    price_returns = price_returns[:-1]\n",
    "\n",
    "image_path_list = image_path_list[:len(price_returns)]\n",
    "\n",
    "assert len(dataset_windows) == len(price_returns) == len(curr_prices) == len(future_prices) == len(image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, batch_size, train_gen, valid_gen, test_gen, gru=False):\n",
    "    \"\"\"Standard training function used by all three models\"\"\" \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # loop through the dataset num_epoch times\n",
    "    for epoch in range(num_epochs):\n",
    "               \n",
    "        # train loop\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        # take the batch and labels for batch \n",
    "        for batch, labels in train_gen:     \n",
    "            if gru:\n",
    "                # add extra dimension to every vector in batch\n",
    "                batch.unsqueeze_(-1)\n",
    "                batch = batch.expand(batch.shape[0], batch.shape[1], 1)\n",
    "                \n",
    "                # reformat dimensions\n",
    "                batch = batch.transpose(2,0)\n",
    "                batch = batch.transpose(1, 2)\n",
    "                \n",
    "            batch, labels = batch.cuda(), labels.cuda()\n",
    "            batch, labels = batch.float(), labels.float()\n",
    "            \n",
    "            # clear gradients\n",
    "            model.zero_grad()\n",
    "            output = model(batch)\n",
    "            \n",
    "            # change the output from whatever matrix to a vector\n",
    "            if gru:\n",
    "                output = output[0]\n",
    "                \n",
    "            output = output.flatten()\n",
    "            \n",
    "            # we use the RMSE error function to train our model\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            \n",
    "            loss = torch.sqrt(criterion(output, labels))\n",
    "            \n",
    "            # backpropogate loss through model\n",
    "            loss.backward()\n",
    "\n",
    "            # perform model training based on propogated loss\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss)\n",
    "            \n",
    "        # validation loop\n",
    "        \n",
    "        profit = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch, labels in valid_gen:\n",
    "                if gru:\n",
    "                    # add extra dimension to every vector in batch\n",
    "                    batch.unsqueeze_(-1)\n",
    "                    batch = batch.expand(batch.shape[0], batch.shape[1], 1)\n",
    "\n",
    "                    # reformat dimensions\n",
    "                    batch = batch.transpose(2,0)\n",
    "                    batch = batch.transpose(1, 2)\n",
    "                    \n",
    "                batch, labels = batch.cuda(), labels.cuda()\n",
    "                batch, labels = batch.float(), labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "\n",
    "                output = model(batch)\n",
    "                \n",
    "                if gru:\n",
    "                    output = output[0] # turn (1, batch_size, 1) to (batch_size, 1)\n",
    "                \n",
    "                output = output.flatten() # turn (batch_size, 1) to (batch_size)\n",
    "                \n",
    "                val_loss = torch.sqrt(criterion(output, labels))\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                valid_loss.append(val_loss)\n",
    "                \n",
    "            \n",
    "            # Profitability testing\n",
    "            profit = 0.0\n",
    "            \n",
    "            for batch, labels in test_gen:\n",
    "                if gru:\n",
    "                    # add extra dimension to every vector in batch\n",
    "                    batch.unsqueeze_(-1)\n",
    "                    batch = batch.expand(batch.shape[0], batch.shape[1], 1)\n",
    "\n",
    "                    # reformat dimensions\n",
    "                    batch = batch.transpose(2,0)\n",
    "                    batch = batch.transpose(1, 2)\n",
    "                \n",
    "                batch, labels = batch.cuda(), labels.cuda()\n",
    "                batch, labels = batch.float(), labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "                \n",
    "                output = model(batch)\n",
    "                \n",
    "                if gru:\n",
    "                    output = output[0] # turn (1, batch_size, 1) to (batch_size, 1)\n",
    "                \n",
    "                output = output.flatten() # turn (batch_size, 1) to (batch_size)\n",
    "                \n",
    "                # if output is > 0 ==> model predict positive growth for the next five cycles. Purchase now and sell in 5 periods.\n",
    "                for i, pred in enumerate(output):\n",
    "                    if pred > 0: # price will increase\n",
    "                        profit += labels[i]\n",
    "                       \n",
    "                model.train()\n",
    "                \n",
    "                \n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "              \"Training Loss: {}\".format(round(float(sum(train_loss)/len(train_loss)), 4)),\n",
    "              \"Validation Loss: {}\".format(round(float(sum(valid_loss)/len(valid_loss)), 4)),\n",
    "              \"Profitability: {}\".format(round(float(profit), 3)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dual(model, num_epochs, batch_size, train_gen1, train_gen2, valid_gen1, valid_gen2, test_gen, gru=False):\n",
    "    \"\"\"Standard training function used by all three models\"\"\"\n",
    "    # For optimizing our model, we choose SGD \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    # training loop\n",
    "    \n",
    "    # toop through the dataset num_epoch times\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train loop\n",
    "        \n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        # loop through each batch\n",
    "        for i  in range(batch_size):\n",
    "            gru_batch, gru_labels = next(iter(train_gen1))\n",
    "            gru_batch, gru_labels = gru_batch.cuda(), gru_labels.cuda()\n",
    "            gru_batch, gru_labels = gru_batch.float(), gru_labels.float()\n",
    "            \n",
    "            # add extra dimension to every vector in batch\n",
    "            gru_batch.unsqueeze_(-1)\n",
    "            gru_batch = gru_batch.expand(gru_batch.shape[0], gru_batch.shape[1], 1)\n",
    "\n",
    "            # reformat dimensions\n",
    "            gru_batch = gru_batch.transpose(2,0)\n",
    "            gru_batch = gru_batch.transpose(1, 2)\n",
    "            cnn_batch, cnn_labels = next(iter(train_gen2))\n",
    "            cnn_batch, cnn_labels = cnn_batch.cuda(), cnn_labels.cuda()\n",
    "            cnn_batch, cnn_labels = cnn_batch.float(), cnn_labels.float()\n",
    "            \n",
    "            # clear gradients\n",
    "            model.zero_grad()\n",
    "            output = model(gru_batch, cnn_batch)\n",
    "            output = output[0]\n",
    "            # declare the loss function and calculate output loss\n",
    "            \n",
    "            # we use the RMSE error function to train our model\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            \n",
    "            loss = torch.sqrt(criterion(output, gru_labels))\n",
    "            \n",
    "            # backpropogate loss through model\n",
    "            loss.backward()\n",
    "            # perform model training based on propogated loss\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss)\n",
    "        \n",
    "        \n",
    "        # validation loop\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for i in range(batch_size):\n",
    "                gru_batch, gru_labels = next(iter(valid_gen1))\n",
    "                gru_batch, gru_labels = gru_batch.cuda(), gru_labels.cuda()\n",
    "                gru_batch, gru_labels = gru_batch.float(), gru_labels.float()\n",
    "                \n",
    "                # add extra dimension to every vector in batch\n",
    "                gru_batch.unsqueeze_(-1)\n",
    "                gru_batch = gru_batch.expand(gru_batch.shape[0], gru_batch.shape[1], 1)\n",
    "\n",
    "                # reformat dimensions\n",
    "                gru_batch = gru_batch.transpose(2,0)\n",
    "                gru_batch = gru_batch.transpose(1, 2)\n",
    "\n",
    "                cnn_batch, cnn_labels = next(iter(valid_gen2))\n",
    "                cnn_batch, cnn_labels = cnn_batch.cuda(), cnn_labels.cuda()\n",
    "                cnn_batch, cnn_labels = cnn_batch.float(), cnn_labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "\n",
    "                output = model(gru_batch, cnn_batch)\n",
    "                output = output[0]\n",
    "                \n",
    "                val_loss = torch.sqrt(criterion(output, gru_labels))\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                valid_loss.append(val_loss)\n",
    "                \n",
    "             # Profitability testing\n",
    "            profit = 0.0\n",
    "            \n",
    "            for batch, labels in test_gen:\n",
    "                gru_batch, gru_labels = next(iter(valid_gen1))\n",
    "                gru_batch, gru_labels = gru_batch.cuda(), gru_labels.cuda()\n",
    "                gru_batch, gru_labels = gru_batch.float(), gru_labels.float()\n",
    "                \n",
    "                # add extra dimension to every vector in batch\n",
    "                gru_batch.unsqueeze_(-1)\n",
    "                gru_batch = gru_batch.expand(gru_batch.shape[0], gru_batch.shape[1], 1)\n",
    "\n",
    "                # reformat dimensions\n",
    "                gru_batch = gru_batch.transpose(2,0)\n",
    "                gru_batch = gru_batch.transpose(1, 2)\n",
    "\n",
    "                cnn_batch, cnn_labels = next(iter(valid_gen2))\n",
    "                cnn_batch, cnn_labels = cnn_batch.cuda(), cnn_labels.cuda()\n",
    "                cnn_batch, cnn_labels = cnn_batch.float(), cnn_labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "\n",
    "                output = model(gru_batch, cnn_batch)\n",
    "                output = output[0]\n",
    "                \n",
    "                # if output is > 0 ==> model predict positive growth for the next five cycles. Purchase now and sell in 5 periods.\n",
    "                \n",
    "                for i, pred in enumerate(output):\n",
    "                    if pred > 0: # price will increase\n",
    "                        profit += labels[i]\n",
    "                \n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "              \"Training Loss: {}\".format(round(float(sum(train_loss)/len(train_loss)), 4)),\n",
    "              \"Validation Loss: {}\".format(round(float(sum(valid_loss)/len(valid_loss)), 4)),\n",
    "              \"Profitability: {}\".format(round(float(profit), 3)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the split between train_df and valid_df from the process of splitting dataset_windows \n",
    "split = 0.7\n",
    "\n",
    "s = int(len(dataset_windows) * 0.7)\n",
    "while s % params['batch_size'] != 0:\n",
    "    s += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two ChartImageDatasets, split by split, for the purpose of creating a DataLoader for the specific model\n",
    "\n",
    "train_ds_cnn = ChartImageDataset(image_path_list[:s], price_returns[:s])\n",
    "valid_ds_cnn = ChartImageDataset(image_path_list[s:], price_returns[s:])\n",
    "\n",
    "# add potential profit as label\n",
    "test_ds_cnn = ChartImageDataset(image_path_list[s:], [future_prices[i] - curr_prices[i] for i in range(s, len(future_prices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_cnn = DataLoader(train_ds_cnn, **params)\n",
    "valid_gen_cnn = DataLoader(valid_ds_cnn, **params)\n",
    "train_gen_cnn = DataLoader(valid_ds_cnn, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_path = Path('models/CNN/cnn_weights')\n",
    "load_model(cnn, cnn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(cnn, num_epochs, batch_size=params['batch_size'], train_gen=train_gen_cnn, valid_gen=valid_gen_cnn, test_gen=train_gen_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(cnn, cnn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_gru = ArrayTimeSeriesDataset(dataset_windows[:s], price_returns[:s])\n",
    "valid_ds_gru = ArrayTimeSeriesDataset(dataset_windows[s:], price_returns[s:])\n",
    "test_ds_gru = ArrayTimeSeriesDataset(dataset_windows[s:], \n",
    "                                     [future_prices[i] - curr_prices[i] for i in range(s, len(future_prices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 800\n",
    "\n",
    "train_gen_gru = DataLoader(train_ds_gru, **params)\n",
    "valid_gen_gru = DataLoader(valid_ds_gru, **params)\n",
    "test_gen_gru = DataLoader(valid_ds_gru, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRUnet(num_features=390, batch_size=params['batch_size'], hidden_size=hidden_size).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_path = Path('models/GRU/gru_weights')\n",
    "load_model(gru, gru_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(gru, num_epochs, batch_size=params['batch_size'], train_gen=train_gen_gru, valid_gen=valid_gen_gru, test_gen=test_gen_gru, gru=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(gru, gru_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GRU-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cnn = GRU_CNN(num_features=390, batch_size=params['batch_size'], hidden_size=800).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cnn_path = Path('models/CNN_GRU/gcnn_gru_weights')\n",
    "load_model(gru_cnn, gru_cnn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cnn.load_cnn_weights(cnn)\n",
    "gru_cnn.load_gru_weights(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dual(gru_cnn, num_epochs, batch_size=params['batch_size'], train_gen1=train_gen_gru, train_gen2=train_gen_cnn,\n",
    "           valid_gen1=valid_gen_gru, valid_gen2=valid_gen_cnn, test_gen=test_gen_gru, gru=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(gru_cnn, gru_cnn_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
