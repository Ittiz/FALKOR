{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image as pil_image\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from strategy.TINNStrategy import prepare_data\n",
    "from chartobjects.Charting import Charting\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import relu\n",
    "from torch.backends import cudnn\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'data/Stocks'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533772800000</td>\n",
       "      <td>0.056560</td>\n",
       "      <td>0.056674</td>\n",
       "      <td>0.056560</td>\n",
       "      <td>0.056621</td>\n",
       "      <td>56.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1533772860000</td>\n",
       "      <td>0.056621</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.056558</td>\n",
       "      <td>0.056607</td>\n",
       "      <td>202.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1533772920000</td>\n",
       "      <td>0.056601</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>0.056599</td>\n",
       "      <td>0.056649</td>\n",
       "      <td>29.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1533772980000</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056673</td>\n",
       "      <td>0.056628</td>\n",
       "      <td>0.056652</td>\n",
       "      <td>32.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1533773040000</td>\n",
       "      <td>0.056639</td>\n",
       "      <td>0.056691</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056675</td>\n",
       "      <td>256.550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time      open      high       low     close   volume\n",
       "0  1533772800000  0.056560  0.056674  0.056560  0.056621   56.604\n",
       "1  1533772860000  0.056621  0.056624  0.056558  0.056607  202.197\n",
       "2  1533772920000  0.056601  0.056668  0.056599  0.056649   29.063\n",
       "3  1533772980000  0.056638  0.056673  0.056628  0.056652   32.376\n",
       "4  1533773040000  0.056639  0.056691  0.056638  0.056675  256.550"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv('data/ETHBTC.csv')[:5000]\n",
    "dataset_df.reset_index(drop=True, inplace=True)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### refactoring steps to get dataset_df into shape [time, open, high, low, close, volume]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['OpenInt'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8f33c8f25b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OpenInt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'open'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'high'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'low'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'volume'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtimestamp_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdte\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamp_col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4095\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4096\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4097\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4098\u001b[0m         )\n\u001b[1;32m   4099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3913\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3915\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3945\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3946\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3947\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3948\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5333\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found in axis\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5334\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5335\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['OpenInt'] not found in axis\""
     ]
    }
   ],
   "source": [
    "dataset_df = dataset_df.drop('OpenInt', axis=1)\n",
    "dataset_df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "timestamp_col = [datetime.timestamp(datetime.strptime(dte, '%Y-%m-%d')) for dte in dataset_df.date]\n",
    "dataset_df.date = timestamp_col\n",
    "dataset_df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "dataset_df.to_csv('data/Stocks/aapl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533772800000</td>\n",
       "      <td>0.056560</td>\n",
       "      <td>0.056674</td>\n",
       "      <td>0.056560</td>\n",
       "      <td>0.056621</td>\n",
       "      <td>56.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1533772860000</td>\n",
       "      <td>0.056621</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.056558</td>\n",
       "      <td>0.056607</td>\n",
       "      <td>202.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1533772920000</td>\n",
       "      <td>0.056601</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>0.056599</td>\n",
       "      <td>0.056649</td>\n",
       "      <td>29.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1533772980000</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056673</td>\n",
       "      <td>0.056628</td>\n",
       "      <td>0.056652</td>\n",
       "      <td>32.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1533773040000</td>\n",
       "      <td>0.056639</td>\n",
       "      <td>0.056691</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056675</td>\n",
       "      <td>256.550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time      open      high       low     close   volume\n",
       "0  1533772800000  0.056560  0.056674  0.056560  0.056621   56.604\n",
       "1  1533772860000  0.056621  0.056624  0.056558  0.056607  202.197\n",
       "2  1533772920000  0.056601  0.056668  0.056599  0.056649   29.063\n",
       "3  1533772980000  0.056638  0.056673  0.056628  0.056652   32.376\n",
       "4  1533773040000  0.056639  0.056691  0.056638  0.056675  256.550"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add technical indicators as feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = prepare_data(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(at, total):\n",
    "    \"\"\"Clears cell output and prints percentage progress\"\"\"\n",
    "    clear_output()\n",
    "    print(\"progress: {}%\".format(round((at/total)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset_df, a=0, b=30, step_size=5):\n",
    "    \"\"\"Split dataset_df into a list of DataFrames. Sliding window of b-a. step_size = 5\n",
    "    returns [ [DataFrame], [DataFrame], ..., [DataFrame] ] where length is dependent on a, b, and step_size. \n",
    "    \"\"\"\n",
    "    \n",
    "    end_b = dataset_df.shape[0] \n",
    "    \n",
    "    dataset_splits = []\n",
    "    \n",
    "    # Take a 30 period window of dataset_df, with a step size of 5\n",
    "    while b < end_b:\n",
    "        if b % 10000 == 0: print_progress(b, end_b)\n",
    "        \n",
    "        window = dataset_df.iloc[a:b, :]\n",
    "        dataset_splits.append(window)\n",
    "        \n",
    "        a += step_size\n",
    "        b += step_size\n",
    "    # dataset_splits = dataset_splits[:len(dataset_splits)-5] # remove last 5 element since we predict price t+5\n",
    "    return dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_columns(split_dataset, cols_list, a=0, b=30, step_size=5):\n",
    "    \"\"\"Split columns dataset_df into a list of DataFrames. Sliding window of b-a. step_size = 5\n",
    "     returns [ [log_col1, log_col2, ...], [log_col1, log_col2, ...], ..., [log_col1, log_col2, ...] ] \n",
    "     where length is dependent on a, b, and step_size. \n",
    "    \"\"\"\n",
    "    \n",
    "    end_b = dataset_df.shape[0] \n",
    "    \n",
    "    columns_splits = []\n",
    "    \n",
    "    # Take a 30 period window of dataset_df, with a step size of 5\n",
    "    while b < end_b-5:\n",
    "        cols = {}\n",
    "        \n",
    "        # init cols dict to empty lists\n",
    "        for col_name in cols_list:\n",
    "            cols[col_name] = []\n",
    "        \n",
    "        for col_name in cols_list:\n",
    "            val = dataset_df.get_value(b, col_name)\n",
    "            cols[col_name].append(val)\n",
    "        \n",
    "        columns_splits.append(cols)\n",
    "                \n",
    "        if b % 10000 == 0: print_progress(b, end_b) \n",
    "            \n",
    "        a += 5\n",
    "        b += 5\n",
    "        \n",
    "    return columns_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_labels(dataset_windows, period_size):\n",
    "    \"\"\"returns a list with len = len(dataset_windows) - 1 containing the price return of time + 5 and now\"\"\"\n",
    "    dct = {'curr_price': [], 'future_price': [], 'return': []}\n",
    "    for i, df in enumerate(dataset_windows[:-1]): # skip the last one\n",
    "        curr_price = df['close'][period_size-1]\n",
    "        dct['curr_price'].append(curr_price)\n",
    "        \n",
    "        future_price = dataset_windows[i+1]['close'][4] # 4 periods into the future\n",
    "        dct['future_price'].append(future_price) \n",
    "        dct['return'].append((future_price) - (curr_price)/curr_price) # take labels as percentage return\n",
    "        \n",
    "    return dct     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"Save a trained PyTorch model to path\"\"\"\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    \"\"\"Load weights into model from path\"\"\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    except:\n",
    "        print('file not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom PyTorch Array Dataset for stock time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayTimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset for historical timeseries data. \n",
    "    self.feature_dfs = [np.Array, np.Array, np.Array, ..., np.Array]\n",
    "    self.labels = [np.Array, np.Array, np.Array, ..., np.Array]\n",
    "    \"\"\"\n",
    "    def __init__(self, time_series, labels):\n",
    "        self.time_series, self.labels = time_series, labels\n",
    "        self.c = 1 # one label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.time_series)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        time_series_arr =  np.array(self.time_series[i])\n",
    "        label = np.array(self.labels[i])\n",
    "        return time_series_arr.flatten(), label.flatten() # convert array into vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset for fusion chart images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChartImageDataset(Dataset):\n",
    "    \"\"\"Stock chart image Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths: list, labels: list):\n",
    "        \"\"\" \n",
    "        image_paths: list containing path to image. Order is maintained\n",
    "        labels: list containing label for each image\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return Tensor representation of image at images_paths[index]\"\"\"\n",
    "        img = pil_image.open(self.image_paths[index])\n",
    "        img.load()\n",
    "        \n",
    "        img_tensor = torchvision.transforms.ToTensor()(img)\n",
    "        \n",
    "        # remove alpha dimension from png\n",
    "        img_tensor = img_tensor[:3,:,:]\n",
    "        return img_tensor, np.array(self.labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU recurrent neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note about dropout layers:\n",
    "You do not need to remove the Dropout layers in testing but you need to call model.eval() before testing. Calling this will change the behavior of layers such as Dropout, BatchNorm, etc. so that Dropout layers, for example, will not affect the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet for stock fusion chart images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training out our convolutional neural network, we instead take a pretrained resnet18 model and apply transfer learning. This method is faster and is more accurate with a limited dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the model by setting up the layers\"\"\"\n",
    "        super(ST_CNN, self).__init__()\n",
    "        \n",
    "        # initial layer is resnet\n",
    "        self.resnet = models.resnet34(pretrained=True, progress=False)\n",
    "        \n",
    "        # final fully connected layers\n",
    "        self.dense1 = nn.Linear(1000, 500)\n",
    "        self.dense2 = nn.Linear(500, 100)\n",
    "        self.dense3 = nn.Linear(100, 12)\n",
    "        \n",
    "        # output layer\n",
    "        self.dense4 = nn.Linear(12, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform a forward pass of our model on some input and hidden state\"\"\"\n",
    "        \n",
    "        x = self.resnet(x)\n",
    "        \n",
    "         # apply three fully-connected Linear layers with ReLU activation function\n",
    "        x = self.dense1(x)\n",
    "        x = relu(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = relu(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = relu(x)\n",
    "        \n",
    "        # output is a size 1 Tensor\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU-CNN model architecture comprised of both resnet18 and grunet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create forward() hooks for grunet and resnet at link layer to recieve their weight tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_CNN(nn.Module):\n",
    "    def __init__(self, num_features, batch_size, hidden_size):\n",
    "        \"\"\"Initialize the model by setting up the layers\"\"\"\n",
    "        super(GRU_CNN, self).__init__()\n",
    "        \n",
    "        # initialize gru and cnn - the full models\n",
    "        \n",
    "        # gru model params\n",
    "        self.num_features = num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        # resnet model\n",
    "        self.cnn = models.resnet34(pretrained=True, progress=False)\n",
    "              \n",
    "        # RNN-GRU model\n",
    "        self.rnn = nn.GRU(input_size=self.num_features,\n",
    "                          hidden_size=self.hidden_size)\n",
    "        \n",
    "        # init GRU hidden layer\n",
    "        self.hidden = self.init_hidden(batch_size=self.batch_size, hidden_size=hidden_size)\n",
    "        self.gru_output = nn.Linear(self.hidden_size, 1000)\n",
    "        \n",
    "        # final fully connected layers\n",
    "        self.dense1 = nn.Linear(1000, 500)\n",
    "        self.dense2 = nn.Linear(500, 100)\n",
    "        self.dense3 = nn.Linear(100, 12)\n",
    "        \n",
    "        # output layer\n",
    "        self.dense4 = nn.Linear(12, 1)\n",
    "    \n",
    "    def load_cnn_weights(self, cnn):\n",
    "        cnn_params = cnn.named_parameters()\n",
    "        gru_cnn_params = dict(self.cnn.named_parameters())\n",
    "        \n",
    "        for name, cnn_param in cnn_params:\n",
    "            if name in gru_cnn_params:\n",
    "                gru_cnn_params[name].data.copy_(cnn_param.data)\n",
    "    \n",
    "    def load_gru_weights(self, gru):\n",
    "        gru_params = gru.named_parameters()\n",
    "        gru_cnn_params = dict(self.rnn.named_parameters())\n",
    "        \n",
    "        for name, gru_param in gru_params:\n",
    "            if name in gru_cnn_params:\n",
    "                gru_cnn_params[name].data.copy_(gru_param.data)\n",
    "    \n",
    "    def forward(self, gru_input, cnn_input):\n",
    "        \"\"\"Perform a forward pass of our model on some input and hidden state\"\"\"\n",
    "  \n",
    "        # gru\n",
    "        gru_out, self.hidden = self.rnn(gru_input, self.hidden)\n",
    "        \n",
    "        # detatch the hidden layer to prevent further backpropagating. i.e. fix the vanishing gradient problem\n",
    "        self.hidden = self.hidden.detach().cuda()\n",
    "        \n",
    "        # pass through linear layer\n",
    "        gru_out = torch.squeeze(self.gru_output(gru_out))\n",
    "                \n",
    "        # cnn\n",
    "        cnn_out = self.cnn(cnn_input)\n",
    "        \n",
    "        # add the outputs of grunet and cnn\n",
    "        x = gru_out.add(cnn_out)\n",
    "        \n",
    "        # feed through final layers\n",
    "\n",
    "        # apply three fully-connected Linear layers with ReLU activation function\n",
    "        x = self.dense1(x)\n",
    "        x = relu(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = relu(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = relu(x)\n",
    "        \n",
    "        # output is a size 1 Tensor\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        \"\"\"Initializes hidden state\"\"\"\n",
    "        \n",
    "        # Creates initial hidden state for GRU of zeroes\n",
    "        hidden = torch.ones(1, self.batch_size, hidden_size).cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select what technical indicators to draw alongside the price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_ti = ['sma10', 'bb10_low', 'bb10_mid', 'bb10_up', 'bb20_low', 'bb20_mid', 'bb20_up'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add price and volume returns as columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append price return and volume return as columns\n",
    "price_return = np.log(dataset_df.close) - np.log(dataset_df.close.shift(1))\n",
    "volume_return = np.log(dataset_df.close) - np.log(dataset_df.close.shift(1))   \n",
    "\n",
    "dataset_df['price_return'] = price_return\n",
    "dataset_df['volume_return'] = volume_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create a list of training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma20</th>\n",
       "      <th>sma50</th>\n",
       "      <th>macd</th>\n",
       "      <th>obv</th>\n",
       "      <th>bb20_low</th>\n",
       "      <th>bb20_mid</th>\n",
       "      <th>bb20_up</th>\n",
       "      <th>price_return</th>\n",
       "      <th>volume_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533774540000</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056750</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>191.736</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.039828</td>\n",
       "      <td>103.084</td>\n",
       "      <td>0.056564</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.056845</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1533774600000</td>\n",
       "      <td>0.056750</td>\n",
       "      <td>0.056774</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056767</td>\n",
       "      <td>55.468</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>158.552</td>\n",
       "      <td>0.056582</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1533774660000</td>\n",
       "      <td>0.056767</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>46.758</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>-0.016565</td>\n",
       "      <td>111.794</td>\n",
       "      <td>0.056614</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.056833</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>-0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1533774720000</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>85.016</td>\n",
       "      <td>0.056730</td>\n",
       "      <td>0.056730</td>\n",
       "      <td>-0.041657</td>\n",
       "      <td>26.778</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.056730</td>\n",
       "      <td>0.056824</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>-0.000776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1533774780000</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.056710</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.056669</td>\n",
       "      <td>64.724</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>-0.064864</td>\n",
       "      <td>-37.946</td>\n",
       "      <td>0.056648</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.056817</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time      open      high       low     close   volume     sma20  \\\n",
       "0  1533774540000  0.056729  0.056750  0.056722  0.056746  191.736  0.056705   \n",
       "1  1533774600000  0.056750  0.056774  0.056722  0.056767   55.468  0.056713   \n",
       "2  1533774660000  0.056767  0.056778  0.056722  0.056722   46.758  0.056724   \n",
       "3  1533774720000  0.056722  0.056735  0.056678  0.056678   85.016  0.056730   \n",
       "4  1533774780000  0.056689  0.056710  0.056667  0.056669   64.724  0.056732   \n",
       "\n",
       "      sma50      macd      obv  bb20_low  bb20_mid   bb20_up  price_return  \\\n",
       "0  0.056705  0.039828  103.084  0.056564  0.056705  0.056845      0.000035   \n",
       "1  0.056713  0.010549  158.552  0.056582  0.056713  0.056843      0.000370   \n",
       "2  0.056724 -0.016565  111.794  0.056614  0.056724  0.056833     -0.000793   \n",
       "3  0.056730 -0.041657   26.778  0.056635  0.056730  0.056824     -0.000776   \n",
       "4  0.056732 -0.064864  -37.946  0.056648  0.056732  0.056817     -0.000159   \n",
       "\n",
       "   volume_return  \n",
       "0       0.000035  \n",
       "1       0.000370  \n",
       "2      -0.000793  \n",
       "3      -0.000776  \n",
       "4      -0.000159  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset_df into slices split by window size of 30 and step_size 5\n",
    "dataset_windows = split_dataset(dataset_df, a=0, b=30, step_size=5)\n",
    "\n",
    "for df in dataset_windows: df.reset_index(drop=True, inplace=True) # reindex 0-29\n",
    "    \n",
    "dataset_windows[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating fusion candlestick, volume, and technical indicator chart images for training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to training images and testing images\n",
    "\n",
    "train_img_path = Path('data/training-images/')\n",
    "test_img_path = Path('data/testing-images/')\n",
    "\n",
    "image_path_list = [train_img_path / 'image-{}.png'.format(i) for i in range(len(dataset_windows))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note - after creating images they will print inside the cell. Restart notebook to clear memory, and rerun, ignoring this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_chart_image(i):\n",
    "    df = dataset_windows[i] # grab the i'th window of the df\n",
    "    chart = Charting(df=df, col_label='time', row_label='close', tech_inds=chart_ti)\n",
    "    chart.chart_to_image(train_img_path / 'image-{}.png'.format(i)) # the / is a Path join method \n",
    "\n",
    "# save every DataFrame of price + vol + tech. id. data into a chart image\n",
    "p = multiprocessing.Pool(processes = 7)\n",
    "\n",
    "p.map_async(generate_chart_image, [i for i in range(len(dataset_windows))])\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_labels_dct = price_labels(dataset_windows, period_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_returns = price_labels_dct['return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n"
     ]
    }
   ],
   "source": [
    "# ensure len(dataset_windows) == len(labels_for_windows)\n",
    "dataset_windows = dataset_windows[:len(price_returns)]\n",
    "print(len(dataset_windows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we normalize our data by taking price returns instead of raw numbers to generalize better, and we log the rest of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma20</th>\n",
       "      <th>sma50</th>\n",
       "      <th>macd</th>\n",
       "      <th>obv</th>\n",
       "      <th>bb20_low</th>\n",
       "      <th>bb20_mid</th>\n",
       "      <th>bb20_up</th>\n",
       "      <th>price_return</th>\n",
       "      <th>volume_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533774240000</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.056736</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.056728</td>\n",
       "      <td>134.008</td>\n",
       "      <td>0.056682</td>\n",
       "      <td>0.056682</td>\n",
       "      <td>0.224341</td>\n",
       "      <td>-70.763</td>\n",
       "      <td>0.056520</td>\n",
       "      <td>0.056682</td>\n",
       "      <td>0.056844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1533774300000</td>\n",
       "      <td>0.056708</td>\n",
       "      <td>0.056725</td>\n",
       "      <td>0.056676</td>\n",
       "      <td>0.056693</td>\n",
       "      <td>100.215</td>\n",
       "      <td>0.056685</td>\n",
       "      <td>0.056685</td>\n",
       "      <td>0.181786</td>\n",
       "      <td>-170.978</td>\n",
       "      <td>0.056522</td>\n",
       "      <td>0.056685</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>-0.000617</td>\n",
       "      <td>-0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1533774360000</td>\n",
       "      <td>0.056693</td>\n",
       "      <td>0.056731</td>\n",
       "      <td>0.056677</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>35.151</td>\n",
       "      <td>0.056684</td>\n",
       "      <td>0.056684</td>\n",
       "      <td>0.142234</td>\n",
       "      <td>-135.827</td>\n",
       "      <td>0.056521</td>\n",
       "      <td>0.056684</td>\n",
       "      <td>0.056846</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1533774420000</td>\n",
       "      <td>0.056731</td>\n",
       "      <td>0.056747</td>\n",
       "      <td>0.056712</td>\n",
       "      <td>0.056721</td>\n",
       "      <td>24.480</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.105505</td>\n",
       "      <td>-111.347</td>\n",
       "      <td>0.056532</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.056847</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1533774480000</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.056723</td>\n",
       "      <td>0.056744</td>\n",
       "      <td>22.695</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>0.071426</td>\n",
       "      <td>-88.652</td>\n",
       "      <td>0.056544</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1533774540000</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056750</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>191.736</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.039828</td>\n",
       "      <td>103.084</td>\n",
       "      <td>0.056564</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.056845</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1533774600000</td>\n",
       "      <td>0.056750</td>\n",
       "      <td>0.056774</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056767</td>\n",
       "      <td>55.468</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>158.552</td>\n",
       "      <td>0.056582</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1533774660000</td>\n",
       "      <td>0.056767</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>46.758</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>-0.016565</td>\n",
       "      <td>111.794</td>\n",
       "      <td>0.056614</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.056833</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>-0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1533774720000</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>85.016</td>\n",
       "      <td>0.056730</td>\n",
       "      <td>0.056730</td>\n",
       "      <td>-0.041657</td>\n",
       "      <td>26.778</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.056730</td>\n",
       "      <td>0.056824</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>-0.000776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1533774780000</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.056710</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.056669</td>\n",
       "      <td>64.724</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>-0.064864</td>\n",
       "      <td>-37.946</td>\n",
       "      <td>0.056648</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.056817</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1533774840000</td>\n",
       "      <td>0.056687</td>\n",
       "      <td>0.056731</td>\n",
       "      <td>0.056660</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>33.872</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>-0.086315</td>\n",
       "      <td>-4.074</td>\n",
       "      <td>0.056666</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.056804</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1533774900000</td>\n",
       "      <td>0.056711</td>\n",
       "      <td>0.056775</td>\n",
       "      <td>0.056695</td>\n",
       "      <td>0.056721</td>\n",
       "      <td>54.452</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>-0.106139</td>\n",
       "      <td>50.378</td>\n",
       "      <td>0.056666</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.056805</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1533774960000</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056733</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056731</td>\n",
       "      <td>30.257</td>\n",
       "      <td>0.056733</td>\n",
       "      <td>0.056733</td>\n",
       "      <td>-0.124453</td>\n",
       "      <td>80.635</td>\n",
       "      <td>0.056665</td>\n",
       "      <td>0.056733</td>\n",
       "      <td>0.056802</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1533775020000</td>\n",
       "      <td>0.056731</td>\n",
       "      <td>0.056733</td>\n",
       "      <td>0.056704</td>\n",
       "      <td>0.056704</td>\n",
       "      <td>13.532</td>\n",
       "      <td>0.056734</td>\n",
       "      <td>0.056734</td>\n",
       "      <td>-0.141368</td>\n",
       "      <td>67.103</td>\n",
       "      <td>0.056665</td>\n",
       "      <td>0.056734</td>\n",
       "      <td>0.056802</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1533775080000</td>\n",
       "      <td>0.056704</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056648</td>\n",
       "      <td>0.056648</td>\n",
       "      <td>26.182</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>-0.156982</td>\n",
       "      <td>40.921</td>\n",
       "      <td>0.056665</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056793</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>-0.000988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1533775140000</td>\n",
       "      <td>0.056652</td>\n",
       "      <td>0.056722</td>\n",
       "      <td>0.056622</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>34.163</td>\n",
       "      <td>0.056723</td>\n",
       "      <td>0.056723</td>\n",
       "      <td>-0.171389</td>\n",
       "      <td>75.084</td>\n",
       "      <td>0.056653</td>\n",
       "      <td>0.056723</td>\n",
       "      <td>0.056792</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1533775200000</td>\n",
       "      <td>0.056697</td>\n",
       "      <td>0.056718</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>0.056670</td>\n",
       "      <td>14.647</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>-0.184683</td>\n",
       "      <td>60.437</td>\n",
       "      <td>0.056656</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>0.056783</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>-0.000864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1533775260000</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>32.901</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>-0.196944</td>\n",
       "      <td>93.338</td>\n",
       "      <td>0.056654</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>0.056773</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1533775320000</td>\n",
       "      <td>0.056659</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>0.056652</td>\n",
       "      <td>0.056652</td>\n",
       "      <td>41.917</td>\n",
       "      <td>0.056710</td>\n",
       "      <td>0.056710</td>\n",
       "      <td>-0.208250</td>\n",
       "      <td>51.421</td>\n",
       "      <td>0.056652</td>\n",
       "      <td>0.056710</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1533775380000</td>\n",
       "      <td>0.056650</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>0.056631</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>48.694</td>\n",
       "      <td>0.056707</td>\n",
       "      <td>0.056707</td>\n",
       "      <td>-0.218672</td>\n",
       "      <td>100.115</td>\n",
       "      <td>0.056643</td>\n",
       "      <td>0.056707</td>\n",
       "      <td>0.056771</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1533775440000</td>\n",
       "      <td>0.056674</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>0.056659</td>\n",
       "      <td>0.056670</td>\n",
       "      <td>99.953</td>\n",
       "      <td>0.056704</td>\n",
       "      <td>0.056704</td>\n",
       "      <td>-0.228279</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.056704</td>\n",
       "      <td>0.056768</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1533775500000</td>\n",
       "      <td>0.056670</td>\n",
       "      <td>0.056720</td>\n",
       "      <td>0.056670</td>\n",
       "      <td>0.056711</td>\n",
       "      <td>114.050</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>-0.237134</td>\n",
       "      <td>114.212</td>\n",
       "      <td>0.056637</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056766</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1533775560000</td>\n",
       "      <td>0.056692</td>\n",
       "      <td>0.056772</td>\n",
       "      <td>0.056690</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>44.180</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>-0.245296</td>\n",
       "      <td>158.392</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.056766</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1533775620000</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>0.056721</td>\n",
       "      <td>124.223</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>-0.252819</td>\n",
       "      <td>34.169</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>0.056767</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1533775680000</td>\n",
       "      <td>0.056721</td>\n",
       "      <td>0.056721</td>\n",
       "      <td>0.056698</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>65.159</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>-0.259753</td>\n",
       "      <td>-30.990</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>0.056767</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1533775740000</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056698</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>46.740</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>-0.266142</td>\n",
       "      <td>15.750</td>\n",
       "      <td>0.056639</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1533775800000</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056699</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>114.265</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>-0.272029</td>\n",
       "      <td>15.750</td>\n",
       "      <td>0.056640</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1533775860000</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056784</td>\n",
       "      <td>0.056727</td>\n",
       "      <td>0.056767</td>\n",
       "      <td>72.824</td>\n",
       "      <td>0.056698</td>\n",
       "      <td>0.056698</td>\n",
       "      <td>-0.277453</td>\n",
       "      <td>88.574</td>\n",
       "      <td>0.056645</td>\n",
       "      <td>0.056698</td>\n",
       "      <td>0.056752</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1533775920000</td>\n",
       "      <td>0.056772</td>\n",
       "      <td>0.056798</td>\n",
       "      <td>0.056756</td>\n",
       "      <td>0.056771</td>\n",
       "      <td>138.847</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>-0.282453</td>\n",
       "      <td>227.421</td>\n",
       "      <td>0.056640</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056761</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1533775980000</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>0.056816</td>\n",
       "      <td>0.056756</td>\n",
       "      <td>0.056816</td>\n",
       "      <td>151.653</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>-0.287061</td>\n",
       "      <td>379.074</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.056772</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             time      open      high       low     close   volume     sma20  \\\n",
       "0   1533774240000  0.056735  0.056736  0.056624  0.056728  134.008  0.056682   \n",
       "1   1533774300000  0.056708  0.056725  0.056676  0.056693  100.215  0.056685   \n",
       "2   1533774360000  0.056693  0.056731  0.056677  0.056716   35.151  0.056684   \n",
       "3   1533774420000  0.056731  0.056747  0.056712  0.056721   24.480  0.056689   \n",
       "4   1533774480000  0.056732  0.056746  0.056723  0.056744   22.695  0.056696   \n",
       "5   1533774540000  0.056729  0.056750  0.056722  0.056746  191.736  0.056705   \n",
       "6   1533774600000  0.056750  0.056774  0.056722  0.056767   55.468  0.056713   \n",
       "7   1533774660000  0.056767  0.056778  0.056722  0.056722   46.758  0.056724   \n",
       "8   1533774720000  0.056722  0.056735  0.056678  0.056678   85.016  0.056730   \n",
       "9   1533774780000  0.056689  0.056710  0.056667  0.056669   64.724  0.056732   \n",
       "10  1533774840000  0.056687  0.056731  0.056660  0.056696   33.872  0.056735   \n",
       "11  1533774900000  0.056711  0.056775  0.056695  0.056721   54.452  0.056735   \n",
       "12  1533774960000  0.056701  0.056733  0.056701  0.056731   30.257  0.056733   \n",
       "13  1533775020000  0.056731  0.056733  0.056704  0.056704   13.532  0.056734   \n",
       "14  1533775080000  0.056704  0.056722  0.056648  0.056648   26.182  0.056729   \n",
       "15  1533775140000  0.056652  0.056722  0.056622  0.056719   34.163  0.056723   \n",
       "16  1533775200000  0.056697  0.056718  0.056668  0.056670   14.647  0.056719   \n",
       "17  1533775260000  0.056680  0.056689  0.056632  0.056680   32.901  0.056714   \n",
       "18  1533775320000  0.056659  0.056680  0.056652  0.056652   41.917  0.056710   \n",
       "19  1533775380000  0.056650  0.056680  0.056631  0.056680   48.694  0.056707   \n",
       "20  1533775440000  0.056674  0.056680  0.056659  0.056670   99.953  0.056704   \n",
       "21  1533775500000  0.056670  0.056720  0.056670  0.056711  114.050  0.056701   \n",
       "22  1533775560000  0.056692  0.056772  0.056690  0.056724   44.180  0.056702   \n",
       "23  1533775620000  0.056719  0.056729  0.056719  0.056721  124.223  0.056703   \n",
       "24  1533775680000  0.056721  0.056721  0.056698  0.056714   65.159  0.056703   \n",
       "25  1533775740000  0.056700  0.056729  0.056698  0.056729   46.740  0.056701   \n",
       "26  1533775800000  0.056729  0.056729  0.056699  0.056729  114.265  0.056700   \n",
       "27  1533775860000  0.056729  0.056784  0.056727  0.056767   72.824  0.056698   \n",
       "28  1533775920000  0.056772  0.056798  0.056756  0.056771  138.847  0.056701   \n",
       "29  1533775980000  0.056760  0.056816  0.056756  0.056816  151.653  0.056705   \n",
       "\n",
       "       sma50      macd      obv  bb20_low  bb20_mid   bb20_up  price_return  \\\n",
       "0   0.056682  0.224341  -70.763  0.056520  0.056682  0.056844           NaN   \n",
       "1   0.056685  0.181786 -170.978  0.056522  0.056685  0.056848     -0.000617   \n",
       "2   0.056684  0.142234 -135.827  0.056521  0.056684  0.056846      0.000406   \n",
       "3   0.056689  0.105505 -111.347  0.056532  0.056689  0.056847      0.000088   \n",
       "4   0.056696  0.071426  -88.652  0.056544  0.056696  0.056848      0.000405   \n",
       "5   0.056705  0.039828  103.084  0.056564  0.056705  0.056845      0.000035   \n",
       "6   0.056713  0.010549  158.552  0.056582  0.056713  0.056843      0.000370   \n",
       "7   0.056724 -0.016565  111.794  0.056614  0.056724  0.056833     -0.000793   \n",
       "8   0.056730 -0.041657   26.778  0.056635  0.056730  0.056824     -0.000776   \n",
       "9   0.056732 -0.064864  -37.946  0.056648  0.056732  0.056817     -0.000159   \n",
       "10  0.056735 -0.086315   -4.074  0.056666  0.056735  0.056804      0.000476   \n",
       "11  0.056735 -0.106139   50.378  0.056666  0.056735  0.056805      0.000441   \n",
       "12  0.056733 -0.124453   80.635  0.056665  0.056733  0.056802      0.000176   \n",
       "13  0.056734 -0.141368   67.103  0.056665  0.056734  0.056802     -0.000476   \n",
       "14  0.056729 -0.156982   40.921  0.056665  0.056729  0.056793     -0.000988   \n",
       "15  0.056723 -0.171389   75.084  0.056653  0.056723  0.056792      0.001253   \n",
       "16  0.056719 -0.184683   60.437  0.056656  0.056719  0.056783     -0.000864   \n",
       "17  0.056714 -0.196944   93.338  0.056654  0.056714  0.056773      0.000176   \n",
       "18  0.056710 -0.208250   51.421  0.056652  0.056710  0.056769     -0.000494   \n",
       "19  0.056707 -0.218672  100.115  0.056643  0.056707  0.056771      0.000494   \n",
       "20  0.056704 -0.228279    0.162  0.056641  0.056704  0.056768     -0.000176   \n",
       "21  0.056701 -0.237134  114.212  0.056637  0.056701  0.056766      0.000723   \n",
       "22  0.056702 -0.245296  158.392  0.056638  0.056702  0.056766      0.000229   \n",
       "23  0.056703 -0.252819   34.169  0.056638  0.056703  0.056767     -0.000053   \n",
       "24  0.056703 -0.259753  -30.990  0.056638  0.056703  0.056767     -0.000123   \n",
       "25  0.056701 -0.266142   15.750  0.056639  0.056701  0.056763      0.000264   \n",
       "26  0.056700 -0.272029   15.750  0.056640  0.056700  0.056760      0.000000   \n",
       "27  0.056698 -0.277453   88.574  0.056645  0.056698  0.056752      0.000670   \n",
       "28  0.056701 -0.282453  227.421  0.056640  0.056701  0.056761      0.000070   \n",
       "29  0.056705 -0.287061  379.074  0.056638  0.056705  0.056772      0.000792   \n",
       "\n",
       "    volume_return  \n",
       "0             NaN  \n",
       "1       -0.000617  \n",
       "2        0.000406  \n",
       "3        0.000088  \n",
       "4        0.000405  \n",
       "5        0.000035  \n",
       "6        0.000370  \n",
       "7       -0.000793  \n",
       "8       -0.000776  \n",
       "9       -0.000159  \n",
       "10       0.000476  \n",
       "11       0.000441  \n",
       "12       0.000176  \n",
       "13      -0.000476  \n",
       "14      -0.000988  \n",
       "15       0.001253  \n",
       "16      -0.000864  \n",
       "17       0.000176  \n",
       "18      -0.000494  \n",
       "19       0.000494  \n",
       "20      -0.000176  \n",
       "21       0.000723  \n",
       "22       0.000229  \n",
       "23      -0.000053  \n",
       "24      -0.000123  \n",
       "25       0.000264  \n",
       "26       0.000000  \n",
       "27       0.000670  \n",
       "28       0.000070  \n",
       "29       0.000792  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_windows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop OCHL data and normalize values by taking log(abs(value)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(i):\n",
    "    df = dataset_windows[i]\n",
    "    if 'time' in df: df = df.drop('time', axis=1)\n",
    "    if 'open' in df: df = df.drop('open', axis=1)\n",
    "    if 'high' in df: df = df.drop('high', axis=1)\n",
    "    if 'low' in df: df = df.drop('low', axis=1)\n",
    "    if 'close' in df: df = df.drop('close', axis=1)\n",
    "    \n",
    "    df.iloc[:, :df.shape[1]-1] = np.log(df.iloc[:, :df.shape[1]-1] + 1) #Remove OHCL columns. Log +1 every value \n",
    "    df = df.fillna(0) # replace all NaN with 0. \n",
    "   \n",
    "    dataset_windows[i] = df\n",
    "    \n",
    "for i in range(len(dataset_windows)): normalize_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>sma20</th>\n",
       "      <th>sma50</th>\n",
       "      <th>macd</th>\n",
       "      <th>obv</th>\n",
       "      <th>bb20_low</th>\n",
       "      <th>bb20_mid</th>\n",
       "      <th>bb20_up</th>\n",
       "      <th>price_return</th>\n",
       "      <th>volume_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.261321</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.039055</td>\n",
       "      <td>4.645198</td>\n",
       "      <td>0.055023</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055288</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.033674</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>5.072370</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055287</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.866147</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>4.725563</td>\n",
       "      <td>0.055070</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>-0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.454533</td>\n",
       "      <td>0.055179</td>\n",
       "      <td>0.055179</td>\n",
       "      <td>-0.042550</td>\n",
       "      <td>3.324244</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.055179</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>-0.000776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.185464</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>-0.067063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055101</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055261</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.551684</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>-0.090270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.015518</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>-0.112205</td>\n",
       "      <td>3.939210</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.442243</td>\n",
       "      <td>0.055182</td>\n",
       "      <td>0.055182</td>\n",
       "      <td>-0.132906</td>\n",
       "      <td>4.402258</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.055182</td>\n",
       "      <td>0.055247</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.676353</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>-0.152414</td>\n",
       "      <td>4.221021</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.055247</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.302555</td>\n",
       "      <td>0.055178</td>\n",
       "      <td>0.055178</td>\n",
       "      <td>-0.170767</td>\n",
       "      <td>3.735787</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.055178</td>\n",
       "      <td>0.055239</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.000988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.559994</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>-0.188005</td>\n",
       "      <td>4.331838</td>\n",
       "      <td>0.055107</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.055238</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.750279</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>-0.204179</td>\n",
       "      <td>4.118012</td>\n",
       "      <td>0.055109</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-0.000864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.523445</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>-0.219331</td>\n",
       "      <td>4.546884</td>\n",
       "      <td>0.055108</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.055220</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.759268</td>\n",
       "      <td>0.055161</td>\n",
       "      <td>0.055161</td>\n",
       "      <td>-0.233510</td>\n",
       "      <td>3.959307</td>\n",
       "      <td>0.055105</td>\n",
       "      <td>0.055161</td>\n",
       "      <td>0.055216</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.905884</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>-0.246760</td>\n",
       "      <td>4.616258</td>\n",
       "      <td>0.055097</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.055218</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.614655</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>-0.259133</td>\n",
       "      <td>0.150143</td>\n",
       "      <td>0.055095</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.745367</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>-0.270672</td>\n",
       "      <td>4.746774</td>\n",
       "      <td>0.055091</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055213</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.810655</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>-0.281429</td>\n",
       "      <td>5.071367</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.830096</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>-0.291448</td>\n",
       "      <td>3.560165</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.192061</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>-0.300771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.865770</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>-0.309440</td>\n",
       "      <td>2.818398</td>\n",
       "      <td>0.055093</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055211</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.747234</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>-0.317494</td>\n",
       "      <td>2.818398</td>\n",
       "      <td>0.055094</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.301684</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>-0.324973</td>\n",
       "      <td>4.495065</td>\n",
       "      <td>0.055099</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.940549</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>-0.331917</td>\n",
       "      <td>5.431190</td>\n",
       "      <td>0.055094</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055209</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.028167</td>\n",
       "      <td>0.055156</td>\n",
       "      <td>0.055156</td>\n",
       "      <td>-0.338359</td>\n",
       "      <td>5.940366</td>\n",
       "      <td>0.055093</td>\n",
       "      <td>0.055156</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.872830</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>-0.344338</td>\n",
       "      <td>5.895468</td>\n",
       "      <td>0.055087</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055239</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.972233</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>6.029459</td>\n",
       "      <td>0.055084</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.055252</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.845433</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>-0.355020</td>\n",
       "      <td>5.989665</td>\n",
       "      <td>0.055077</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055269</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>-0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.932260</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>-0.359781</td>\n",
       "      <td>5.944140</td>\n",
       "      <td>0.055073</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.055281</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.531631</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>-0.364187</td>\n",
       "      <td>5.913333</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055288</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      volume     sma20     sma50      macd       obv  bb20_low  bb20_mid  \\\n",
       "0   5.261321  0.055155  0.055155  0.039055  4.645198  0.055023  0.055155   \n",
       "1   4.033674  0.055163  0.055163  0.010494  5.072370  0.055039  0.055163   \n",
       "2   3.866147  0.055173  0.055173 -0.016704  4.725563  0.055070  0.055173   \n",
       "3   4.454533  0.055179  0.055179 -0.042550  3.324244  0.055089  0.055179   \n",
       "4   4.185464  0.055181  0.055181 -0.067063  0.000000  0.055101  0.055181   \n",
       "5   3.551684  0.055184  0.055184 -0.090270  0.000000  0.055119  0.055184   \n",
       "6   4.015518  0.055184  0.055184 -0.112205  3.939210  0.055119  0.055184   \n",
       "7   3.442243  0.055182  0.055182 -0.132906  4.402258  0.055118  0.055182   \n",
       "8   2.676353  0.055183  0.055183 -0.152414  4.221021  0.055118  0.055183   \n",
       "9   3.302555  0.055178  0.055178 -0.170767  3.735787  0.055118  0.055178   \n",
       "10  3.559994  0.055172  0.055172 -0.188005  4.331838  0.055107  0.055172   \n",
       "11  2.750279  0.055169  0.055169 -0.204179  4.118012  0.055109  0.055169   \n",
       "12  3.523445  0.055164  0.055164 -0.219331  4.546884  0.055108  0.055164   \n",
       "13  3.759268  0.055161  0.055161 -0.233510  3.959307  0.055105  0.055161   \n",
       "14  3.905884  0.055158  0.055158 -0.246760  4.616258  0.055097  0.055158   \n",
       "15  4.614655  0.055155  0.055155 -0.259133  0.150143  0.055095  0.055155   \n",
       "16  4.745367  0.055152  0.055152 -0.270672  4.746774  0.055091  0.055152   \n",
       "17  3.810655  0.055153  0.055153 -0.281429  5.071367  0.055092  0.055153   \n",
       "18  4.830096  0.055153  0.055153 -0.291448  3.560165  0.055092  0.055153   \n",
       "19  4.192061  0.055153  0.055153 -0.300771  0.000000  0.055092  0.055153   \n",
       "20  3.865770  0.055152  0.055152 -0.309440  2.818398  0.055093  0.055152   \n",
       "21  4.747234  0.055151  0.055151 -0.317494  2.818398  0.055094  0.055151   \n",
       "22  4.301684  0.055149  0.055149 -0.324973  4.495065  0.055099  0.055149   \n",
       "23  4.940549  0.055151  0.055151 -0.331917  5.431190  0.055094  0.055151   \n",
       "24  5.028167  0.055156  0.055156 -0.338359  5.940366  0.055093  0.055156   \n",
       "25  2.872830  0.055163  0.055163 -0.344338  5.895468  0.055087  0.055163   \n",
       "26  3.972233  0.055168  0.055168 -0.349881  6.029459  0.055084  0.055168   \n",
       "27  2.845433  0.055173  0.055173 -0.355020  5.989665  0.055077  0.055173   \n",
       "28  2.932260  0.055177  0.055177 -0.359781  5.944140  0.055073  0.055177   \n",
       "29  2.531631  0.055181  0.055181 -0.364187  5.913333  0.055075  0.055181   \n",
       "\n",
       "     bb20_up  price_return  volume_return  \n",
       "0   0.055288      0.000035       0.000035  \n",
       "1   0.055287      0.000370       0.000370  \n",
       "2   0.055277     -0.000793      -0.000793  \n",
       "3   0.055268     -0.000776      -0.000776  \n",
       "4   0.055261     -0.000159      -0.000159  \n",
       "5   0.055250      0.000476       0.000476  \n",
       "6   0.055250      0.000441       0.000441  \n",
       "7   0.055247      0.000176       0.000176  \n",
       "8   0.055247     -0.000476      -0.000476  \n",
       "9   0.055239     -0.000989      -0.000988  \n",
       "10  0.055238      0.001252       0.001253  \n",
       "11  0.055229     -0.000865      -0.000864  \n",
       "12  0.055220      0.000176       0.000176  \n",
       "13  0.055216     -0.000494      -0.000494  \n",
       "14  0.055218      0.000494       0.000494  \n",
       "15  0.055215     -0.000176      -0.000176  \n",
       "16  0.055213      0.000723       0.000723  \n",
       "17  0.055214      0.000229       0.000229  \n",
       "18  0.055214     -0.000053      -0.000053  \n",
       "19  0.055214     -0.000123      -0.000123  \n",
       "20  0.055211      0.000264       0.000264  \n",
       "21  0.055208      0.000000       0.000000  \n",
       "22  0.055200      0.000669       0.000670  \n",
       "23  0.055209      0.000070       0.000070  \n",
       "24  0.055219      0.000792       0.000792  \n",
       "25  0.055239     -0.000229      -0.000229  \n",
       "26  0.055252      0.000475       0.000475  \n",
       "27  0.055269     -0.000194      -0.000194  \n",
       "28  0.055281     -0.000458      -0.000458  \n",
       "29  0.055288     -0.000070      -0.000070  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_windows[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all NaN values - in our case only the first window has a NaN so we remove first window and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not done:\n",
    "    dataset_windows = dataset_windows[1:]\n",
    "    price_returns = price_returns[1:] \n",
    "    curr_prices = price_labels_dct['curr_price'][1:]\n",
    "    future_prices = price_labels_dct['future_price'][1:]\n",
    "    done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 5}\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice datasets to fit with batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>sma20</th>\n",
       "      <th>sma50</th>\n",
       "      <th>macd</th>\n",
       "      <th>obv</th>\n",
       "      <th>bb20_low</th>\n",
       "      <th>bb20_mid</th>\n",
       "      <th>bb20_up</th>\n",
       "      <th>price_return</th>\n",
       "      <th>volume_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.261321</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.039055</td>\n",
       "      <td>4.645198</td>\n",
       "      <td>0.055023</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055288</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.033674</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>5.072370</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055287</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.866147</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>4.725563</td>\n",
       "      <td>0.055070</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>-0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.454533</td>\n",
       "      <td>0.055179</td>\n",
       "      <td>0.055179</td>\n",
       "      <td>-0.042550</td>\n",
       "      <td>3.324244</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.055179</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>-0.000776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.185464</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>-0.067063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055101</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055261</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.551684</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>-0.090270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.015518</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>-0.112205</td>\n",
       "      <td>3.939210</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.442243</td>\n",
       "      <td>0.055182</td>\n",
       "      <td>0.055182</td>\n",
       "      <td>-0.132906</td>\n",
       "      <td>4.402258</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.055182</td>\n",
       "      <td>0.055247</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.676353</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>-0.152414</td>\n",
       "      <td>4.221021</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.055247</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.302555</td>\n",
       "      <td>0.055178</td>\n",
       "      <td>0.055178</td>\n",
       "      <td>-0.170767</td>\n",
       "      <td>3.735787</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.055178</td>\n",
       "      <td>0.055239</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.000988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.559994</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>-0.188005</td>\n",
       "      <td>4.331838</td>\n",
       "      <td>0.055107</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.055238</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.750279</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>-0.204179</td>\n",
       "      <td>4.118012</td>\n",
       "      <td>0.055109</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-0.000864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.523445</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>-0.219331</td>\n",
       "      <td>4.546884</td>\n",
       "      <td>0.055108</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.055220</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.759268</td>\n",
       "      <td>0.055161</td>\n",
       "      <td>0.055161</td>\n",
       "      <td>-0.233510</td>\n",
       "      <td>3.959307</td>\n",
       "      <td>0.055105</td>\n",
       "      <td>0.055161</td>\n",
       "      <td>0.055216</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.905884</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>-0.246760</td>\n",
       "      <td>4.616258</td>\n",
       "      <td>0.055097</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.055218</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.614655</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>-0.259133</td>\n",
       "      <td>0.150143</td>\n",
       "      <td>0.055095</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.745367</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>-0.270672</td>\n",
       "      <td>4.746774</td>\n",
       "      <td>0.055091</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055213</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.810655</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>-0.281429</td>\n",
       "      <td>5.071367</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.830096</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>-0.291448</td>\n",
       "      <td>3.560165</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.192061</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>-0.300771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.865770</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>-0.309440</td>\n",
       "      <td>2.818398</td>\n",
       "      <td>0.055093</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.055211</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.747234</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>-0.317494</td>\n",
       "      <td>2.818398</td>\n",
       "      <td>0.055094</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.301684</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>-0.324973</td>\n",
       "      <td>4.495065</td>\n",
       "      <td>0.055099</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.940549</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>-0.331917</td>\n",
       "      <td>5.431190</td>\n",
       "      <td>0.055094</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.055209</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.028167</td>\n",
       "      <td>0.055156</td>\n",
       "      <td>0.055156</td>\n",
       "      <td>-0.338359</td>\n",
       "      <td>5.940366</td>\n",
       "      <td>0.055093</td>\n",
       "      <td>0.055156</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.872830</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>-0.344338</td>\n",
       "      <td>5.895468</td>\n",
       "      <td>0.055087</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.055239</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.972233</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>-0.349881</td>\n",
       "      <td>6.029459</td>\n",
       "      <td>0.055084</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.055252</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.845433</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>-0.355020</td>\n",
       "      <td>5.989665</td>\n",
       "      <td>0.055077</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.055269</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>-0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.932260</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>-0.359781</td>\n",
       "      <td>5.944140</td>\n",
       "      <td>0.055073</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.055281</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.531631</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>-0.364187</td>\n",
       "      <td>5.913333</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.055181</td>\n",
       "      <td>0.055288</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      volume     sma20     sma50      macd       obv  bb20_low  bb20_mid  \\\n",
       "0   5.261321  0.055155  0.055155  0.039055  4.645198  0.055023  0.055155   \n",
       "1   4.033674  0.055163  0.055163  0.010494  5.072370  0.055039  0.055163   \n",
       "2   3.866147  0.055173  0.055173 -0.016704  4.725563  0.055070  0.055173   \n",
       "3   4.454533  0.055179  0.055179 -0.042550  3.324244  0.055089  0.055179   \n",
       "4   4.185464  0.055181  0.055181 -0.067063  0.000000  0.055101  0.055181   \n",
       "5   3.551684  0.055184  0.055184 -0.090270  0.000000  0.055119  0.055184   \n",
       "6   4.015518  0.055184  0.055184 -0.112205  3.939210  0.055119  0.055184   \n",
       "7   3.442243  0.055182  0.055182 -0.132906  4.402258  0.055118  0.055182   \n",
       "8   2.676353  0.055183  0.055183 -0.152414  4.221021  0.055118  0.055183   \n",
       "9   3.302555  0.055178  0.055178 -0.170767  3.735787  0.055118  0.055178   \n",
       "10  3.559994  0.055172  0.055172 -0.188005  4.331838  0.055107  0.055172   \n",
       "11  2.750279  0.055169  0.055169 -0.204179  4.118012  0.055109  0.055169   \n",
       "12  3.523445  0.055164  0.055164 -0.219331  4.546884  0.055108  0.055164   \n",
       "13  3.759268  0.055161  0.055161 -0.233510  3.959307  0.055105  0.055161   \n",
       "14  3.905884  0.055158  0.055158 -0.246760  4.616258  0.055097  0.055158   \n",
       "15  4.614655  0.055155  0.055155 -0.259133  0.150143  0.055095  0.055155   \n",
       "16  4.745367  0.055152  0.055152 -0.270672  4.746774  0.055091  0.055152   \n",
       "17  3.810655  0.055153  0.055153 -0.281429  5.071367  0.055092  0.055153   \n",
       "18  4.830096  0.055153  0.055153 -0.291448  3.560165  0.055092  0.055153   \n",
       "19  4.192061  0.055153  0.055153 -0.300771  0.000000  0.055092  0.055153   \n",
       "20  3.865770  0.055152  0.055152 -0.309440  2.818398  0.055093  0.055152   \n",
       "21  4.747234  0.055151  0.055151 -0.317494  2.818398  0.055094  0.055151   \n",
       "22  4.301684  0.055149  0.055149 -0.324973  4.495065  0.055099  0.055149   \n",
       "23  4.940549  0.055151  0.055151 -0.331917  5.431190  0.055094  0.055151   \n",
       "24  5.028167  0.055156  0.055156 -0.338359  5.940366  0.055093  0.055156   \n",
       "25  2.872830  0.055163  0.055163 -0.344338  5.895468  0.055087  0.055163   \n",
       "26  3.972233  0.055168  0.055168 -0.349881  6.029459  0.055084  0.055168   \n",
       "27  2.845433  0.055173  0.055173 -0.355020  5.989665  0.055077  0.055173   \n",
       "28  2.932260  0.055177  0.055177 -0.359781  5.944140  0.055073  0.055177   \n",
       "29  2.531631  0.055181  0.055181 -0.364187  5.913333  0.055075  0.055181   \n",
       "\n",
       "     bb20_up  price_return  volume_return  \n",
       "0   0.055288      0.000035       0.000035  \n",
       "1   0.055287      0.000370       0.000370  \n",
       "2   0.055277     -0.000793      -0.000793  \n",
       "3   0.055268     -0.000776      -0.000776  \n",
       "4   0.055261     -0.000159      -0.000159  \n",
       "5   0.055250      0.000476       0.000476  \n",
       "6   0.055250      0.000441       0.000441  \n",
       "7   0.055247      0.000176       0.000176  \n",
       "8   0.055247     -0.000476      -0.000476  \n",
       "9   0.055239     -0.000989      -0.000988  \n",
       "10  0.055238      0.001252       0.001253  \n",
       "11  0.055229     -0.000865      -0.000864  \n",
       "12  0.055220      0.000176       0.000176  \n",
       "13  0.055216     -0.000494      -0.000494  \n",
       "14  0.055218      0.000494       0.000494  \n",
       "15  0.055215     -0.000176      -0.000176  \n",
       "16  0.055213      0.000723       0.000723  \n",
       "17  0.055214      0.000229       0.000229  \n",
       "18  0.055214     -0.000053      -0.000053  \n",
       "19  0.055214     -0.000123      -0.000123  \n",
       "20  0.055211      0.000264       0.000264  \n",
       "21  0.055208      0.000000       0.000000  \n",
       "22  0.055200      0.000669       0.000670  \n",
       "23  0.055209      0.000070       0.000070  \n",
       "24  0.055219      0.000792       0.000792  \n",
       "25  0.055239     -0.000229      -0.000229  \n",
       "26  0.055252      0.000475       0.000475  \n",
       "27  0.055269     -0.000194      -0.000194  \n",
       "28  0.055281     -0.000458      -0.000458  \n",
       "29  0.055288     -0.000070      -0.000070  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while len(dataset_windows) % params['batch_size'] != 0:\n",
    "    dataset_windows = dataset_windows[:-1]\n",
    "    \n",
    "while len(curr_prices) % params['batch_size'] != 0:\n",
    "    curr_prices = curr_prices[:-1]\n",
    "\n",
    "while len(future_prices) % params['batch_size'] != 0:\n",
    "    future_prices = future_prices[:-1]\n",
    "\n",
    "while len(price_returns) % params['batch_size'] != 0:\n",
    "    price_returns = price_returns[:-1]\n",
    "  \n",
    "assert len(dataset_windows) == len(price_returns) == len(curr_prices) == len(future_prices)\n",
    "dataset_windows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard training loop with Stochastic Gradient Descent as the backprop algorithm, and RMSE metric for the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, batch_size, train_gen, valid_gen, test_gen, gru=False):\n",
    "    \"\"\"Standard training function used by all three models\"\"\"\n",
    "    \n",
    "    # For optimizing our model, we choose SGD \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "    \n",
    "    # training loop\n",
    "    \n",
    "    # toop through the dataset num_epoch times\n",
    "    for epoch in range(num_epochs):\n",
    "               \n",
    "        # train loop\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        # take the batch and labels for batch \n",
    "        for batch, labels in train_gen:\n",
    "            \n",
    "            if gru:\n",
    "                # add extra dimension to every vector in batch\n",
    "                batch.unsqueeze_(-1)\n",
    "                batch = batch.expand(batch.shape[0], batch.shape[1], 1)\n",
    "                \n",
    "                # reformat dimensions\n",
    "                batch = batch.transpose(2,0)\n",
    "                batch = batch.transpose(1, 2)\n",
    "                \n",
    "            batch, labels = batch.cuda(), labels.cuda()\n",
    "            batch, labels = batch.float(), labels.float()\n",
    "            \n",
    "            # clear gradients\n",
    "            model.zero_grad()\n",
    "            output = model(batch)\n",
    "            \n",
    "            if gru:\n",
    "                output = output[0] # turn (1, batch_size, 1) to (batch_size, 1)\n",
    "            \n",
    "            # declare the loss function and calculate output loss\n",
    "            \n",
    "            # we use the RMSE error function to train our model\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            loss = torch.sqrt(criterion(output, labels))\n",
    "            \n",
    "            # backpropogate loss through model\n",
    "            loss.backward()\n",
    "\n",
    "            # perform model training based on propogated loss\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss)\n",
    "        \n",
    "        # validation loop\n",
    "        \n",
    "        profit = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch, labels in valid_gen:\n",
    "                if gru:\n",
    "                    # add extra dimension to every vector in batch\n",
    "                    batch.unsqueeze_(-1)\n",
    "                    batch = batch.expand(batch.shape[0], batch.shape[1], 1)\n",
    "\n",
    "                    # reformat dimensions\n",
    "                    batch = batch.transpose(2,0)\n",
    "                    batch = batch.transpose(1, 2)\n",
    "                    \n",
    "                batch, labels = batch.cuda(), labels.cuda()\n",
    "                batch, labels = batch.float(), labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "\n",
    "                output = model(batch)\n",
    "                \n",
    "                if gru:\n",
    "                    output = output[0] # turn (1, batch_size, 1) to (batch_size, 1)\n",
    "                \n",
    "                val_loss = torch.sqrt(criterion(output, labels))\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                valid_loss.append(val_loss)\n",
    "                \n",
    "            \n",
    "            # Profitability testing\n",
    "            profit = 0.0\n",
    "            \n",
    "            for batch, labels in test_gen:\n",
    "                if gru:\n",
    "                    # add extra dimension to every vector in batch\n",
    "                    batch.unsqueeze_(-1)\n",
    "                    batch = batch.expand(batch.shape[0], batch.shape[1], 1)\n",
    "\n",
    "                    # reformat dimensions\n",
    "                    batch = batch.transpose(2,0)\n",
    "                    batch = batch.transpose(1, 2)\n",
    "                \n",
    "                batch, labels = batch.cuda(), labels.cuda()\n",
    "                batch, labels = batch.float(), labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "                \n",
    "                output = model(batch)\n",
    "                if gru:\n",
    "                    output = output[0] # turn (1, batch_size, 1) to (batch_size, 1)\n",
    "                \n",
    "                # if output is > 0 ==> model predict positive growth for the next five cycles. Purchase now and sell in 5 periods.\n",
    "                for i, pred in enumerate(output):\n",
    "                    #print(pred)\n",
    "                    if pred[0] > 0: # price will increase\n",
    "                        profit += labels[i]\n",
    "                       \n",
    "                model.train()\n",
    "                \n",
    "                \n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "              \"Training Loss: {}\".format(round(float(sum(train_loss)/len(train_loss)), 4)),\n",
    "              \"Validation Loss: {}\".format(round(float(sum(valid_loss)/len(valid_loss)), 4)),\n",
    "              \"Profitability: {}\".format(round(float(profit), 3)))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop for GRU-CNN with two inputs - chart images and time series array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dual(model, num_epochs, batch_size, train_gen1, train_gen2, valid_gen1, valid_gen2, test_gen, gru=False):\n",
    "    \"\"\"Standard training function used by all three models\"\"\"\n",
    "    # For optimizing our model, we choose SGD \n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    # training loop\n",
    "    \n",
    "    # toop through the dataset num_epoch times\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train loop\n",
    "        \n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        # loop through each batch\n",
    "        for i  in range(batch_size):\n",
    "            gru_batch, gru_labels = next(iter(train_gen1))\n",
    "            gru_batch, gru_labels = gru_batch.cuda(), gru_labels.cuda()\n",
    "            gru_batch, gru_labels = gru_batch.float(), gru_labels.float()\n",
    "            \n",
    "            # add extra dimension to every vector in batch\n",
    "            gru_batch.unsqueeze_(-1)\n",
    "            gru_batch = gru_batch.expand(gru_batch.shape[0], gru_batch.shape[1], 1)\n",
    "\n",
    "            # reformat dimensions\n",
    "            gru_batch = gru_batch.transpose(2,0)\n",
    "            gru_batch = gru_batch.transpose(1, 2)\n",
    "            cnn_batch, cnn_labels = next(iter(train_gen2))\n",
    "            cnn_batch, cnn_labels = cnn_batch.cuda(), cnn_labels.cuda()\n",
    "            cnn_batch, cnn_labels = cnn_batch.float(), cnn_labels.float()\n",
    "            \n",
    "            # clear gradients\n",
    "            model.zero_grad()\n",
    "            output = model(gru_batch, cnn_batch)\n",
    "            output = output[0]\n",
    "            # declare the loss function and calculate output loss\n",
    "            \n",
    "            # we use the RMSE error function to train our model\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            loss = torch.sqrt(criterion(output, gru_labels))\n",
    "            \n",
    "            # backpropogate loss through model\n",
    "            loss.backward()\n",
    "            # perform model training based on propogated loss\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss)\n",
    "        \n",
    "        \n",
    "        # validation loop\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for i in range(batch_size):\n",
    "                gru_batch, gru_labels = next(iter(valid_gen1))\n",
    "                gru_batch, gru_labels = gru_batch.cuda(), gru_labels.cuda()\n",
    "                gru_batch, gru_labels = gru_batch.float(), gru_labels.float()\n",
    "                \n",
    "                # add extra dimension to every vector in batch\n",
    "                gru_batch.unsqueeze_(-1)\n",
    "                gru_batch = gru_batch.expand(gru_batch.shape[0], gru_batch.shape[1], 1)\n",
    "\n",
    "                # reformat dimensions\n",
    "                gru_batch = gru_batch.transpose(2,0)\n",
    "                gru_batch = gru_batch.transpose(1, 2)\n",
    "\n",
    "                cnn_batch, cnn_labels = next(iter(valid_gen2))\n",
    "                cnn_batch, cnn_labels = cnn_batch.cuda(), cnn_labels.cuda()\n",
    "                cnn_batch, cnn_labels = cnn_batch.float(), cnn_labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "\n",
    "                output = model(gru_batch, cnn_batch)\n",
    "                output = output[0]\n",
    "                \n",
    "                val_loss = torch.sqrt(criterion(output, gru_labels))\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                valid_loss.append(val_loss)\n",
    "                \n",
    "             # Profitability testing\n",
    "            profit = 0.0\n",
    "            \n",
    "            for batch, labels in test_gen:\n",
    "                gru_batch, gru_labels = next(iter(valid_gen1))\n",
    "                gru_batch, gru_labels = gru_batch.cuda(), gru_labels.cuda()\n",
    "                gru_batch, gru_labels = gru_batch.float(), gru_labels.float()\n",
    "                \n",
    "                # add extra dimension to every vector in batch\n",
    "                gru_batch.unsqueeze_(-1)\n",
    "                gru_batch = gru_batch.expand(gru_batch.shape[0], gru_batch.shape[1], 1)\n",
    "\n",
    "                # reformat dimensions\n",
    "                gru_batch = gru_batch.transpose(2,0)\n",
    "                gru_batch = gru_batch.transpose(1, 2)\n",
    "\n",
    "                cnn_batch, cnn_labels = next(iter(valid_gen2))\n",
    "                cnn_batch, cnn_labels = cnn_batch.cuda(), cnn_labels.cuda()\n",
    "                cnn_batch, cnn_labels = cnn_batch.float(), cnn_labels.float()\n",
    "                \n",
    "                # transform the model from training configuration to testing configuration. ex. dropout layers are removed\n",
    "                model.eval()\n",
    "\n",
    "                output = model(gru_batch, cnn_batch)\n",
    "                output = output[0]\n",
    "                \n",
    "                # if output is > 0 ==> model predict positive growth for the next five cycles. Purchase now and sell in 5 periods.\n",
    "                \n",
    "                for i, pred in enumerate(output):\n",
    "                    if pred > 0: # price will increase\n",
    "                        profit += labels[i]\n",
    "                \n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "              \"Training Loss: {}\".format(round(float(sum(train_loss)/len(train_loss)), 4)),\n",
    "              \"Validation Loss: {}\".format(round(float(sum(valid_loss)/len(valid_loss)), 4)),\n",
    "              \"Profitability: {}\".format(round(float(profit), 3)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preparing input data for train() by creating DataLoaders for train and valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the split between train_df and valid_df from the process of splitting dataset_windows and labels_for_windows\n",
    "split = 0.7\n",
    "\n",
    "s = int(len(dataset_windows) * 0.7)\n",
    "while s % params['batch_size'] != 0:\n",
    "    s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two ChartImageDatasets, split by split, for the purpose of creating a DataLoader for the specific model\n",
    "\n",
    "train_ds_cnn = ChartImageDataset(image_path_list[:s], price_returns[:s])\n",
    "valid_ds_cnn = ChartImageDataset(image_path_list[s:], price_returns[s:])\n",
    "\n",
    "# add potential profit as label\n",
    "test_ds_cnn = ChartImageDataset(image_path_list[s:], [future_prices[i] - curr_prices[i] for i in range(s, len(future_prices))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_cnn = DataLoader(train_ds_cnn, **params)\n",
    "valid_gen_cnn = DataLoader(valid_ds_cnn, **params)\n",
    "train_gen_cnn = DataLoader(valid_ds_cnn, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ST_CNN().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CNN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_path = Path('strategy/cnn/cnn_weights')\n",
    "load_model(cnn, cnn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Training Loss: 3103.28 Validation Loss: inf Profitability: 0.0\n",
      "Epoch: 2/10... Training Loss: 11.8363 Validation Loss: 1.2226 Profitability: -242.969\n",
      "Epoch: 3/10... Training Loss: 1.1538 Validation Loss: 1.0113 Profitability: -242.969\n",
      "Epoch: 4/10... Training Loss: 0.9022 Validation Loss: 0.7081 Profitability: 0.0\n",
      "Epoch: 5/10... Training Loss: 0.5802 Validation Loss: 0.3606 Profitability: 0.0\n",
      "Epoch: 6/10... Training Loss: 0.2227 Validation Loss: 0.0109 Profitability: 0.0\n",
      "Epoch: 7/10... Training Loss: 0.102 Validation Loss: 0.1978 Profitability: 0.0\n",
      "Epoch: 8/10... Training Loss: 0.2003 Validation Loss: 0.1696 Profitability: 0.0\n",
      "Epoch: 9/10... Training Loss: 0.1167 Validation Loss: 0.0065 Profitability: 0.0\n",
      "Epoch: 10/10... Training Loss: 0.0643 Validation Loss: 0.1257 Profitability: 0.0\n"
     ]
    }
   ],
   "source": [
    "train(cnn, num_epochs, batch_size=params['batch_size'], train_gen=train_gen_cnn, valid_gen=valid_gen_cnn, test_gen=train_gen_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save CNN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(cnn, cnn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the split between train_df and valid_df from the process of splitting dataset_windows and labels_for_windows\n",
    "split = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the inputs by logging each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two ArrayTimeSeriesDatasets, split by split, for the purpose of creating a DataLoader for the specific model\n",
    "\n",
    "s = int(len(dataset_windows) * 0.7)\n",
    "while s % params['batch_size'] != 0:\n",
    "    s += 1\n",
    "\n",
    "train_ds_gru = ArrayTimeSeriesDataset(dataset_windows[:s], price_returns[:s])\n",
    "valid_ds_gru = ArrayTimeSeriesDataset(dataset_windows[s:], price_returns[s:])\n",
    "test_ds_gru = ArrayTimeSeriesDataset(dataset_windows[s:], [future_prices[i] - curr_prices[i] for i in range(s, len(future_prices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 2500\n",
    "\n",
    "train_gen_gru = DataLoader(train_ds_gru, **params)\n",
    "valid_gen_gru = DataLoader(valid_ds_gru, **params)\n",
    "test_gen_gru = DataLoader(valid_ds_gru, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GRU weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRUnet(num_features=300, batch_size=params['batch_size'], hidden_size=hidden_size).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_path = Path('strategy/gru/gru_weights')\n",
    "load_model(gru, gru_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(gru, num_epochs*10, batch_size=params['batch_size'], train_gen=train_gen_gru, valid_gen=valid_gen_gru, test_gen=test_gen_gru, gru=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(gru, gru_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GRU-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = int(len(dataset_windows) * 0.7)\n",
    "while s % params['batch_size'] != 0:\n",
    "    s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cnn = GRU_CNN(num_features=390, batch_size=params['batch_size'], hidden_size=800).float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GRU-CNN Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cnn_path = Path('strategy/cnn-gru/cnn_gru_weights')\n",
    "load_model(gru_cnn, gru_cnn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weights of GRU-CNN with pretrained GRU and CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cnn.load_cnn_weights(cnn)\n",
    "gru_cnn.load_gru_weights(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dual(gru_cnn, num_epochs, batch_size=params['batch_size'], train_gen1=train_gen_gru, train_gen2=train_gen_cnn,\n",
    "           valid_gen1=valid_gen_gru, valid_gen2=valid_gen_cnn, test_gen=test_gen_gru, gru=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save GRU-CNN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(gru_cnn, gru_cnn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
